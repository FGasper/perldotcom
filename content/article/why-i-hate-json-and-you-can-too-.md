
  {
    "title"       : "Why I Hate JSON (and you can, too)",
    "authors"     : ["felipe-gasper"],
    "date"        : "2019-11-10T21:52:51",
    "tags"        : [],
    "draft"       : true,
    "image"       : "",
    "thumbnail"   : "",
    "description" : "Another look at this ubiquitous serialization ‚Ä¶",
    "categories"  : "data"
  }

[JSON](https://www.json.org/) needs no introduction among most programmers.
Since its ‚Äúdiscovery‚Äù by Douglas Crockford in the early 2000s it has become
one of the most popular data serialization formats yet created, having
effectively supplanted XML as the web‚Äôs ‚Äúgo-to‚Äù format for representing
structured data in APIs.

For all its popularity, though, JSON carries a number of liabilities that
make it less than ideal in many contexts where it nonetheless enjoys
widespread use. This article will discuss some of these and propose
alternative formats that I think deserve your attention.


1. UTF-8
========

JSON‚Äôs chief liability is its UTF-8 restriction: all JSON documents must
be UTF-8, from start to finish. This means any application that wants to
exchange binary data either cannot use JSON, or‚Äîperhaps more commonly‚Äîuses
a secondary encoding like Base64 to store binary strings as text.
([Let‚Äôs Encrypt](https://letsencrypt.org)‚Äôs
[ACME](https://tools.ietf.org/html/rfc8555) protocol does this.)
Such techniques carry performance penalties and eventually create consistency
problems, such as in the [WAMP](https://wamp-proto.org/_static/gen/wamp_latest_ietf.html) protocol, where it‚Äôs impossible to distinguish an ‚Äúencoded‚Äù binary
string from a text string that happens to fit the pattern that the protocol
recommends for encoding binary strings to JSON.

UTF-8 is a fine way of encoding text, but applications that use JSON bind
themselves to a restriction that may prove to have been imprudent‚Äîsometimes
only long after an ecosystem has already developed and a serialization change
would be painful.

2. UTF-8, Redux
========

JSON carries the same UTF-8 liability that JavaScript does: when using the
`\uXXXX` notation it represents all
UTF-8 characters via UTF-16 notation. While that‚Äôs fine for applications whose
strings consist solely of characters from the Basic Multilingual Plane
(BMP)‚ÄîUnicode code points below 65,536‚Äîit‚Äôs awkward for representing anything
beyond this range. Unfortunately, given the prevalence of emoji characters,
it‚Äôs not at all unusual to need to represent Unicode characters above the BMP
in a JSON document.

This is possible, of course, but it has to be done as UTF-16 would represent
such a character. This makes it harder for a human to read and write JSON;
how many would intuit that `"\ud83d\udca9"` is actually the pile-of-poo
emoji (üí©, U+1f4a9)?

3. Human Unfriendliness: Comments
=======================

[json.org](https://json.org) describes JSON as ‚Äúeasy for humans to read and
write.‚Äù This is true, but it‚Äôs only part of the picture: while it may be easy
for a human to read and write an isolated JSON document, JSON is an
unfriendly format for teams of humans to _maintain_.

This is true firstly because of the absence of comments. While various
JSON encoders and decoders allow comments, these are not mutually consistent,
and interoperability problems arise when a minor deviation from a global
standard becomes an in-house standard. The absence of comments is particularly
problematic in configuration files, where developers making choices like:

1. Use a nonstandard JSON variant that allows comments. This builds in a
dependency either on specific implementations, which defeats the point of
having a standard format, or on a ‚Äúnormalization‚Äù step in the workflow,
which adds additional complication and maintenance overhead.

2. Alter the data schema to incorporate strings that the application ignores
but that developers can use to annotate the document.

3. Store comments in a separate file, perhaps alongside the JSON.

4. Forgo comments entirely.

None of these is especially inviting. (The ‚Äúnormalization‚Äù
approach‚Äî[Crockford‚Äôs recommendation](https://archive.is/8FWsA)‚Äîseems the
least offensive.) It would be far better to keep things simple by using
a configuration format that allows comments.

4. Human Unfriendliness: Commas
===================

A secondary displeasure that JSON brings is the annoyance at its rejection
of trailing commas. While modern JavaScript engines now tolerate these,
the JSON standard still forbids them, which adds additional overhead when
maintaining JSON documents.

5. Inefficiency: Strings
===============

Every C programmer deals with the two fundamental ways of encoding strings:
either delimit them with a ‚Äúspecial‚Äù character (in C‚Äôs case, the NUL byte)
or transmit their length along with their ‚Äúbody‚Äù.

JSON, being derived from a programming language, uses the former method.
Unlike C strings, of course, JSON provides an escaping mechanism that allows
storage of the delimiter character (for JSON, that‚Äôs `"`). That, however,
entails a significant level of complexity for an encoder: a JSON encoder
cannot encode a string without first inspecting it to look for characters
that need to be escaped.

6. Iffy Standardization
===============

I‚Äôve been linking in this article to [json.org](https://json.org) rather than
a formal
reference for the format because there **is**, in fact, no unified JSON
standard. Both the IETF and ECMA maintain JSON standards‚Äîthe IETF has
issued no fewer than **3** recensions of theirs!

Even putting that aside, JSON has some peculiar encoding nits. Contrary
to widespread belief, JSON is _not_ a subset of JavaScript/ECMAScript
because JSON, unlike JavaScript, permits Unicode line and paragraph
separators (U+2028 and U+2029, respectively).

JSON is also‚Äîagain, contrary to widespread misconception‚Äî_not_ a subset of
[YAML](https://yaml.org/), another widely-used serialization format. YAML,
for example, does not allow the `\/` sequence in encoded strings; this
sequence is common (though not required) in JSON encoders to avoid HTML
encoding problems when JSON is output directly into the body of an HTML
`<script>` tag.

So, JSON is its own little beast.

What to Use Instead?
=======================

I find [TOML](https://github.com/toml-lang/toml) a compelling choice for
configuration files. It fixes JSON‚Äôs UTF-16 problem (by allowing both
`\uXXXX` and `\UXXXXXXXX` formats) and allows for comments.
Projects as diverse as Rust‚Äôs [Cargo](https://doc.rust-lang.org/cargo/)
package manager and [Hugo](https://gohugo.io/)‚Äîwhich powers this site!‚Äîare
some of TOML‚Äôs most prominent users.

For applications I recommend either the IETF‚Äôs [CBOR](https://cbor.io/)
or Booking.com‚Äôs [Sereal](https://github.com/Sereal/Sereal). (Disclaimer:
I wrote 2 of CPAN‚Äôs 3 CBOR libraries.) Both of these
offer improved efficiency compared to JSON and,
critically, can store binary strings. CBOR is probably better for applications
that need to communicate across multiple languages, whereas Sereal boasts
especially good Perl support, including storage of regular
expressions‚Äîwith modifiers!

(Another format, [MessagePack](https://msgpack.org/), is a ‚Äúforerunner‚Äù to
CBOR in many respects. MessagePack remains in wide use but
has only [one CPAN implementation](https://metacpan.org/pod/Data::MessagePack),
which appears to be unmaintained.)

This all having been said: if you are confident that your application
will _never_ need to serialize anything that isn‚Äôt UTF-8, and if human
readability is of greater concern to you than performance, then by all means
use JSON! But be aware, of course, of the limitations you‚Äôre imposing on
yourself and all future maintainers by so doing.
